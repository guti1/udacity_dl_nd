{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with an RNN\n",
    "\n",
    "In this notebook, you'll implement a recurrent neural network that performs sentiment analysis. Using an RNN rather than a feedfoward network is more accurate since we can include information about the *sequence* of words. Here we'll use a dataset of movie reviews, accompanied by labels.\n",
    "\n",
    "The architecture for this network is shown below.\n",
    "\n",
    "<img src=\"assets/network_diagram.png\" width=400px>\n",
    "\n",
    "Here, we'll pass in words to an embedding layer. We need an embedding layer because we have tens of thousands of words, so we'll need a more efficient representation for our input data than one-hot encoded vectors. You should have seen this before from the word2vec lesson. You can actually train up an embedding with word2vec and use it here. But it's good enough to just have an embedding layer and let the network learn the embedding table on it's own.\n",
    "\n",
    "From the embedding layer, the new representations will be passed to LSTM cells. These will add recurrent connections to the network so we can include information about the sequence of words in the data. Finally, the LSTM cells will go to a sigmoid output layer here. We're using the sigmoid because we're trying to predict if this text has positive or negative sentiment. The output layer will just be a single unit then, with a sigmoid activation function.\n",
    "\n",
    "We don't care about the sigmoid outputs except for the very last one, we can ignore the rest. We'll calculate the cost from the output of the last step and the training label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentiment-network/reviews.txt', 'r') as f:\n",
    "    reviews = f.read()\n",
    "with open('sentiment-network/labels.txt', 'r') as f:\n",
    "    labels = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \\nstory of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turned into an insane  violent mob by the crazy chantings of it  s singers . unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting . even those from the era should be turned off . the cryptic dialogue would make shakespeare seem easy to a third grader . on a technical level it  s better than you might think with some good cinematography by future great vilmos zsigmond . future stars sally kirkland and frederic forrest can be seen briefly .  \\nhomelessness  or houselessness as george carlin stated  has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school  work  or vote for the matter . most people think of the homeless as just a lost cause while worrying about things such as racism  the war on iraq  pressuring kids to succeed  technology  the elections  inflation  or worrying if they  ll be next to end up on the streets .  br    br   but what if y'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "The first step when building a neural network model is getting your data into the proper form to feed into the network. Since we're using embedding layers, we'll need to encode each word with an integer. We'll also want to clean it up a bit.\n",
    "\n",
    "You can see an example of the reviews data above. We'll want to get rid of those periods. Also, you might notice that the reviews are delimited with newlines `\\n`. To deal with those, I'm going to split the text into each review using `\\n` as the delimiter. Then I can combined all the reviews back together into one big string.\n",
    "\n",
    "First, let's remove all punctuation. Then get all the text without the newlines and split it into individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "all_text = ''.join([c for c in reviews if c not in punctuation])\n",
    "reviews = all_text.split('\\n')\n",
    "\n",
    "all_text = ' '.join(reviews)\n",
    "words = all_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell high is a cartoon comedy  it ran at the same time as some other programs about school life  such as  teachers   my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers   the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students  when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled          at           high  a classic line inspector i  m here to sack one of your teachers  student welcome to bromwell high  i expect that many adults of my age think that bromwell high is far fetched  what a pity that it isn  t    story of a man who has unnatural feelings for a pig  starts out with a opening scene that is a terrific example of absurd comedy  a formal orchestra audience is turned into an insane  violent mob by the crazy chantings of it  s singers  unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting  even those from the era should be turned off  the cryptic dialogue would make shakespeare seem easy to a third grader  on a technical level it  s better than you might think with some good cinematography by future great vilmos zsigmond  future stars sally kirkland and frederic forrest can be seen briefly    homelessness  or houselessness as george carlin stated  has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school  work  or vote for the matter  most people think of the homeless as just a lost cause while worrying about things such as racism  the war on iraq  pressuring kids to succeed  technology  the elections  inflation  or worrying if they  ll be next to end up on the streets   br    br   but what if you were given a bet to live on the st'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bromwell',\n",
       " 'high',\n",
       " 'is',\n",
       " 'a',\n",
       " 'cartoon',\n",
       " 'comedy',\n",
       " 'it',\n",
       " 'ran',\n",
       " 'at',\n",
       " 'the',\n",
       " 'same',\n",
       " 'time',\n",
       " 'as',\n",
       " 'some',\n",
       " 'other',\n",
       " 'programs',\n",
       " 'about',\n",
       " 'school',\n",
       " 'life',\n",
       " 'such',\n",
       " 'as',\n",
       " 'teachers',\n",
       " 'my',\n",
       " 'years',\n",
       " 'in',\n",
       " 'the',\n",
       " 'teaching',\n",
       " 'profession',\n",
       " 'lead',\n",
       " 'me',\n",
       " 'to',\n",
       " 'believe',\n",
       " 'that',\n",
       " 'bromwell',\n",
       " 'high',\n",
       " 's',\n",
       " 'satire',\n",
       " 'is',\n",
       " 'much',\n",
       " 'closer',\n",
       " 'to',\n",
       " 'reality',\n",
       " 'than',\n",
       " 'is',\n",
       " 'teachers',\n",
       " 'the',\n",
       " 'scramble',\n",
       " 'to',\n",
       " 'survive',\n",
       " 'financially',\n",
       " 'the',\n",
       " 'insightful',\n",
       " 'students',\n",
       " 'who',\n",
       " 'can',\n",
       " 'see',\n",
       " 'right',\n",
       " 'through',\n",
       " 'their',\n",
       " 'pathetic',\n",
       " 'teachers',\n",
       " 'pomp',\n",
       " 'the',\n",
       " 'pettiness',\n",
       " 'of',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'situation',\n",
       " 'all',\n",
       " 'remind',\n",
       " 'me',\n",
       " 'of',\n",
       " 'the',\n",
       " 'schools',\n",
       " 'i',\n",
       " 'knew',\n",
       " 'and',\n",
       " 'their',\n",
       " 'students',\n",
       " 'when',\n",
       " 'i',\n",
       " 'saw',\n",
       " 'the',\n",
       " 'episode',\n",
       " 'in',\n",
       " 'which',\n",
       " 'a',\n",
       " 'student',\n",
       " 'repeatedly',\n",
       " 'tried',\n",
       " 'to',\n",
       " 'burn',\n",
       " 'down',\n",
       " 'the',\n",
       " 'school',\n",
       " 'i',\n",
       " 'immediately',\n",
       " 'recalled',\n",
       " 'at',\n",
       " 'high']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the words\n",
    "\n",
    "The embedding lookup requires that we pass in integers to our network. The easiest way to do this is to create dictionaries that map the words in the vocabulary to integers. Then we can convert each of our reviews into integers so they can be passed into the network.\n",
    "\n",
    "> **Exercise:** Now you're going to encode the words with integers. Build a dictionary that maps words to integers. Later we're going to pad our input vectors with zeros, so make sure the integers **start at 1, not 0**.\n",
    "> Also, convert the reviews to integers and store the reviews in a new list called `reviews_ints`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_ints=[]\n",
    "for review in reviews:\n",
    "    reviews_int.append([vocab_to_int[k] for k in review.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your dictionary that maps vocab words to integers here\n",
    "vocab_to_int = {}\n",
    "reviews_ints=[]\n",
    "\n",
    "i=1\n",
    "words_set = set(words)\n",
    "\n",
    "for word in words_set:\n",
    "    vocab_to_int[word] = i\n",
    "    i+=1\n",
    "\n",
    "# Convert the reviews to integers, same shape as reviews list, but with integers\n",
    "reviews_ints = []\n",
    "for review in reviews:\n",
    "    reviews_int.append([vocab_to_int[k] for k in review.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'curatola': 1,\n",
       " 'mindf': 48575,\n",
       " 'doldrums': 2,\n",
       " 'draughtswoman': 5,\n",
       " 'particulary': 6,\n",
       " 'rca': 7,\n",
       " 'sunlit': 29726,\n",
       " 'bernadette': 8,\n",
       " 'homestead': 9,\n",
       " 'belabored': 10,\n",
       " 'swoops': 11,\n",
       " 'yeats': 12,\n",
       " 'loserto': 15,\n",
       " 'sinise': 24768,\n",
       " 'reaming': 63532,\n",
       " 'unreformable': 18,\n",
       " 'brouhaha': 17,\n",
       " 'stiffed': 19,\n",
       " 'soulplane': 20,\n",
       " 'silva': 22,\n",
       " 'offon': 24,\n",
       " 'warnercolor': 26,\n",
       " 'shipley': 27,\n",
       " 'verikoan': 29,\n",
       " 'olympiad': 30,\n",
       " 'fastly': 31,\n",
       " 'atlee': 4,\n",
       " 'reaking': 30635,\n",
       " 'blasters': 32,\n",
       " 'consoled': 12288,\n",
       " 'conditio': 71379,\n",
       " 'campier': 34,\n",
       " 'espouses': 36,\n",
       " 'cheerful': 35,\n",
       " 'wonders': 37,\n",
       " 'schmo': 40,\n",
       " 'resmblance': 43,\n",
       " 'femi': 5810,\n",
       " 'gaylord': 44,\n",
       " 'dazzle': 46,\n",
       " 'drawing': 47,\n",
       " 'illogical': 48,\n",
       " 'goodwin': 49544,\n",
       " 'flavor': 49,\n",
       " 'vending': 50651,\n",
       " 'exempt': 51,\n",
       " 'marmaduke': 52,\n",
       " 'priyadarshan': 53,\n",
       " 'emand': 55,\n",
       " 'drip': 56,\n",
       " 'polarizing': 58,\n",
       " 'entirely': 59,\n",
       " 'rohleder': 62,\n",
       " 'yorke': 67393,\n",
       " 'unprovoked': 63,\n",
       " 'reveling': 65,\n",
       " 'highjinx': 67,\n",
       " 'cartons': 69,\n",
       " 'oompah': 71,\n",
       " 'melissa': 72,\n",
       " 'sovsem': 61831,\n",
       " 'ranko': 73,\n",
       " 'letup': 49547,\n",
       " 'dreamscapes': 75,\n",
       " 'leashes': 76,\n",
       " 'betsab': 77,\n",
       " 'billingsley': 78,\n",
       " 'kurupt': 80,\n",
       " 'earphone': 82,\n",
       " 'sarno': 83,\n",
       " 'barthelmy': 84,\n",
       " 'faired': 12291,\n",
       " 'parsifals': 19459,\n",
       " 'lyn': 85,\n",
       " 'timeless': 87,\n",
       " 'kushiata': 88,\n",
       " 'ingenuos': 89,\n",
       " 'mconaughey': 90,\n",
       " 'topsy': 91,\n",
       " 'boingo': 92,\n",
       " 'lukes': 93,\n",
       " 'recognizes': 94,\n",
       " 'noam': 96,\n",
       " 'reignite': 24777,\n",
       " 'quinnell': 97,\n",
       " 'adle': 98,\n",
       " 'hempel': 24778,\n",
       " 'naturedly': 99,\n",
       " 'sequestered': 100,\n",
       " 'lancing': 101,\n",
       " 'chautard': 51949,\n",
       " 'temperamental': 102,\n",
       " 'lipo': 105,\n",
       " 'mol': 104,\n",
       " 'guinneapig': 106,\n",
       " 'bubble': 107,\n",
       " 'fluidly': 110,\n",
       " 'definitly': 111,\n",
       " 'foundational': 112,\n",
       " 'bensonhurst': 14,\n",
       " 'wnbq': 44821,\n",
       " 'fanzine': 113,\n",
       " 'ladylike': 114,\n",
       " 'endearments': 115,\n",
       " 'danvers': 117,\n",
       " 'gunnerside': 118,\n",
       " 'shaheen': 120,\n",
       " 'gorged': 56617,\n",
       " 'outmatched': 121,\n",
       " 'cardigan': 68484,\n",
       " 'operatically': 55042,\n",
       " 'badgers': 122,\n",
       " 'ancient': 124,\n",
       " 'rationing': 16,\n",
       " 'potato': 125,\n",
       " 'phibes': 127,\n",
       " 'muncey': 128,\n",
       " 'motions': 129,\n",
       " 'tarring': 130,\n",
       " 'sibling': 65506,\n",
       " 'squalor': 131,\n",
       " 'ennia': 134,\n",
       " 'virgil': 49815,\n",
       " 'marcos': 63778,\n",
       " 'angie': 135,\n",
       " 'healthy': 136,\n",
       " 'vocalise': 50807,\n",
       " 'consciousness': 138,\n",
       " 'dello': 139,\n",
       " 'snickered': 21,\n",
       " 'zwartboek': 24786,\n",
       " 'lauren': 140,\n",
       " 'finessing': 142,\n",
       " 'blatty': 37197,\n",
       " 'harping': 143,\n",
       " 'ellison': 49563,\n",
       " 'dominatrix': 144,\n",
       " 'tailing': 145,\n",
       " 'wealth': 146,\n",
       " 'kerosine': 152,\n",
       " 'colossal': 148,\n",
       " 'spangled': 149,\n",
       " 'personally': 150,\n",
       " 'estates': 151,\n",
       " 'shilling': 153,\n",
       " 'muddling': 155,\n",
       " 'taffy': 156,\n",
       " 'tatou': 157,\n",
       " 'faggus': 158,\n",
       " 'cash': 159,\n",
       " 'humorous': 161,\n",
       " 'bidder': 162,\n",
       " 'anacronisms': 168,\n",
       " 'chesley': 14715,\n",
       " 'galecki': 169,\n",
       " 'stamp': 170,\n",
       " 'interrogator': 28,\n",
       " 'barek': 172,\n",
       " 'densest': 173,\n",
       " 'removing': 175,\n",
       " 'dov': 176,\n",
       " 'germna': 37202,\n",
       " 'geritan': 179,\n",
       " 'charts': 180,\n",
       " 'dystopic': 183,\n",
       " 'lobbing': 184,\n",
       " 'josette': 185,\n",
       " 'composited': 186,\n",
       " 'partition': 187,\n",
       " 'highland': 188,\n",
       " 'pajama': 190,\n",
       " 'ply': 191,\n",
       " 'driftwood': 61852,\n",
       " 'nananana': 193,\n",
       " 'biel': 194,\n",
       " 'peerlessly': 196,\n",
       " 'belyt': 62011,\n",
       " 'kikabidze': 197,\n",
       " 'swigged': 198,\n",
       " 'reoccurring': 199,\n",
       " 'creaky': 24796,\n",
       " 'rappaport': 201,\n",
       " 'kristine': 202,\n",
       " 'acquaintance': 204,\n",
       " 'win': 205,\n",
       " 'dennison': 206,\n",
       " 'bogus': 207,\n",
       " 'perseverence': 209,\n",
       " 'curricular': 211,\n",
       " 'crabtree': 213,\n",
       " 'clevage': 212,\n",
       " 'hornburg': 9137,\n",
       " 'pop': 214,\n",
       " 'thumble': 215,\n",
       " 'swirls': 216,\n",
       " 'childbirth': 217,\n",
       " 'innovatively': 219,\n",
       " 'konchalovsky': 220,\n",
       " 'proms': 222,\n",
       " 'superpowerman': 223,\n",
       " 'bancroft': 24797,\n",
       " 'ciera': 224,\n",
       " 'shaffner': 225,\n",
       " 'massacres': 226,\n",
       " 'meager': 227,\n",
       " 'throats': 229,\n",
       " 'shipka': 12312,\n",
       " 'oreo': 49791,\n",
       " 'thumbs': 33,\n",
       " 'impalings': 230,\n",
       " 'weightier': 61855,\n",
       " 'citroen': 235,\n",
       " 'streisand': 234,\n",
       " 'survived': 233,\n",
       " 'strivings': 237,\n",
       " 'euthanized': 236,\n",
       " 'paras': 238,\n",
       " 'weirdly': 239,\n",
       " 'fantasyland': 240,\n",
       " 'subscribe': 241,\n",
       " 'wackos': 242,\n",
       " 'fedevich': 49574,\n",
       " 'exiting': 243,\n",
       " 'tenuously': 244,\n",
       " 'championship': 245,\n",
       " 'datta': 250,\n",
       " 'pharma': 248,\n",
       " 'multiplying': 249,\n",
       " 'filmcow': 72120,\n",
       " 'almoust': 12317,\n",
       " 'under': 251,\n",
       " 'ancestor': 50520,\n",
       " 'asininity': 252,\n",
       " 'skateboarder': 254,\n",
       " 'gnome': 49577,\n",
       " 'ulees': 67870,\n",
       " 'abounding': 255,\n",
       " 'amalgamated': 36987,\n",
       " 'kelvin': 256,\n",
       " 'vigil': 30560,\n",
       " 'cheesier': 260,\n",
       " 'greasy': 259,\n",
       " 'vanquishes': 45,\n",
       " 'tercero': 266,\n",
       " 'returned': 263,\n",
       " 'wallace': 264,\n",
       " 'anjana': 265,\n",
       " 'uncovering': 24808,\n",
       " 'hygiene': 267,\n",
       " 'tomei': 49580,\n",
       " 'catalan': 269,\n",
       " 'kookoo': 49655,\n",
       " 'statuesque': 270,\n",
       " 'veins': 271,\n",
       " 'grocery': 272,\n",
       " 'munnabhai': 273,\n",
       " 'pregnant': 275,\n",
       " 'convulsively': 277,\n",
       " 'methodically': 278,\n",
       " 'trib': 283,\n",
       " 'ebola': 280,\n",
       " 'penitentiaries': 281,\n",
       " 'populations': 282,\n",
       " 'monomaniacal': 292,\n",
       " 'coffin': 284,\n",
       " 'discursive': 286,\n",
       " 'fogies': 37220,\n",
       " 'eliciting': 287,\n",
       " 'parlablane': 293,\n",
       " 'maiden': 289,\n",
       " 'anarky': 294,\n",
       " 'handshakes': 291,\n",
       " 'spiked': 24813,\n",
       " 'grinchy': 295,\n",
       " 'subtitle': 296,\n",
       " 'shinbei': 297,\n",
       " 'unconcious': 298,\n",
       " 'oust': 61863,\n",
       " 'zhi': 299,\n",
       " 'telesales': 300,\n",
       " 'disillusionment': 301,\n",
       " 'apprehended': 302,\n",
       " 'usercomments': 306,\n",
       " 'clu': 304,\n",
       " 'tiana': 54747,\n",
       " 'stierman': 51558,\n",
       " 'lopardi': 310,\n",
       " 'genteel': 308,\n",
       " 'expendable': 50,\n",
       " 'mallrats': 311,\n",
       " 'blanked': 312,\n",
       " 'lunatic': 313,\n",
       " 'emerlius': 37224,\n",
       " 'garibaldi': 73259,\n",
       " 'team': 24817,\n",
       " 'clouts': 314,\n",
       " 'tmc': 315,\n",
       " 'slated': 316,\n",
       " 'throwaways': 71787,\n",
       " 'cel': 61870,\n",
       " 'timeslip': 317,\n",
       " 'bisto': 318,\n",
       " 'gymnast': 320,\n",
       " 'kaidan': 54,\n",
       " 'gwangi': 321,\n",
       " 'fattened': 61869,\n",
       " 'cynical': 68632,\n",
       " 'thumpers': 322,\n",
       " 'rapacious': 37227,\n",
       " 'performer': 323,\n",
       " 'jerichow': 16701,\n",
       " 'durn': 324,\n",
       " 'daerden': 325,\n",
       " 'overprinting': 326,\n",
       " 'barbara': 61871,\n",
       " 'kitumura': 327,\n",
       " 'indecipherable': 328,\n",
       " 'encapsulating': 329,\n",
       " 'balkanized': 51617,\n",
       " 'kristensen': 61875,\n",
       " 'tabletop': 331,\n",
       " 'vengeant': 37229,\n",
       " 'coselli': 61877,\n",
       " 'joachim': 333,\n",
       " 'iguana': 334,\n",
       " 'complement': 57,\n",
       " 'brion': 335,\n",
       " 'shirts': 336,\n",
       " 'partnership': 337,\n",
       " 'downpour': 338,\n",
       " 'hardline': 339,\n",
       " 'jazzman': 340,\n",
       " 'sonatine': 11105,\n",
       " 'bernson': 341,\n",
       " 'conducting': 67437,\n",
       " 'quintessence': 60,\n",
       " 'stimulate': 342,\n",
       " 'instilling': 41629,\n",
       " 'wholeness': 344,\n",
       " 'kinnison': 345,\n",
       " 'setup': 346,\n",
       " 'damm': 24824,\n",
       " 'remorselessness': 351,\n",
       " 'macgregor': 349,\n",
       " 'consilation': 350,\n",
       " 'knudsen': 352,\n",
       " 'airwolfs': 353,\n",
       " 'ensnare': 354,\n",
       " 'spoons': 64,\n",
       " 'rivkin': 69712,\n",
       " 'scull': 61885,\n",
       " 'ogar': 355,\n",
       " 'niellson': 66,\n",
       " 'barnaby': 357,\n",
       " 'poignance': 358,\n",
       " 'afterwords': 359,\n",
       " 'facie': 360,\n",
       " 'dredge': 72437,\n",
       " 'vashti': 362,\n",
       " 'terrier': 363,\n",
       " 'gekko': 364,\n",
       " 'storied': 73470,\n",
       " 'cranking': 365,\n",
       " 'genuineness': 366,\n",
       " 'schell': 367,\n",
       " 'educate': 368,\n",
       " 'serialkiller': 369,\n",
       " 'robyn': 372,\n",
       " 'evened': 371,\n",
       " 'sears': 374,\n",
       " 'uglier': 373,\n",
       " 'chethe': 375,\n",
       " 'sociopathy': 377,\n",
       " 'proudly': 68,\n",
       " 'assemble': 379,\n",
       " 'jastrow': 381,\n",
       " 'pollutions': 49598,\n",
       " 'menaikkan': 382,\n",
       " 'afterschool': 383,\n",
       " 'englishness': 384,\n",
       " 'leckie': 70,\n",
       " 'doris': 385,\n",
       " 'atm': 386,\n",
       " 'frazzlingly': 387,\n",
       " 'vagaries': 59360,\n",
       " 'dissolve': 388,\n",
       " 'necronomicon': 389,\n",
       " 'confusingly': 390,\n",
       " 'thnik': 391,\n",
       " 'gato': 73028,\n",
       " 'doggett': 397,\n",
       " 'midi': 395,\n",
       " 'benetakos': 12339,\n",
       " 'anticipated': 396,\n",
       " 'immoral': 47141,\n",
       " 'lucci': 12342,\n",
       " 'whatevers': 401,\n",
       " 'caratherisic': 400,\n",
       " 'lifesaver': 49602,\n",
       " 'magazines': 402,\n",
       " 'younes': 403,\n",
       " 'wiz': 404,\n",
       " 'richandson': 405,\n",
       " 'institutional': 406,\n",
       " 'paratrooper': 54302,\n",
       " 'patrician': 407,\n",
       " 'smuttishness': 49604,\n",
       " 'hitoshi': 68638,\n",
       " 'polytheists': 58503,\n",
       " 'faschist': 410,\n",
       " 'airtight': 412,\n",
       " 'crows': 414,\n",
       " 'wrists': 415,\n",
       " 'fubar': 416,\n",
       " 'lamented': 417,\n",
       " 'huckster': 419,\n",
       " 'uneasiness': 420,\n",
       " 'piracy': 421,\n",
       " 'miamis': 422,\n",
       " 'renyolds': 423,\n",
       " 'giants': 424,\n",
       " 'hinkley': 426,\n",
       " 'mccarten': 429,\n",
       " 'chose': 427,\n",
       " 'infantile': 428,\n",
       " 'predominately': 37240,\n",
       " 'blasphemy': 65851,\n",
       " 'tatooine': 79,\n",
       " 'arrondisments': 433,\n",
       " 'familar': 434,\n",
       " 'upper': 435,\n",
       " 'presbyterian': 61901,\n",
       " 'sleuthing': 67923,\n",
       " 'mlc': 436,\n",
       " 'wind': 437,\n",
       " 'hyperactive': 438,\n",
       " 'varhola': 439,\n",
       " 'tomilinson': 440,\n",
       " 'froze': 71540,\n",
       " 'stagehand': 441,\n",
       " 'lamia': 61905,\n",
       " 'coronel': 53050,\n",
       " 'cavalry': 442,\n",
       " 'maternity': 443,\n",
       " 'herzogian': 445,\n",
       " 'durward': 449,\n",
       " 'nicolson': 448,\n",
       " 'mingella': 61904,\n",
       " 'ponto': 453,\n",
       " 'shield': 450,\n",
       " 'betterment': 451,\n",
       " 'fetching': 454,\n",
       " 'keith': 455,\n",
       " 'hacienda': 457,\n",
       " 'reliefs': 12350,\n",
       " 'performanceeven': 460,\n",
       " 'exports': 459,\n",
       " 'regehr': 462,\n",
       " 'unfounded': 463,\n",
       " 'punctuality': 464,\n",
       " 'ickyness': 465,\n",
       " 'mindedly': 466,\n",
       " 'amatuer': 467,\n",
       " 'disgracefully': 468,\n",
       " 'fecundity': 54117,\n",
       " 'brassiere': 49614,\n",
       " 'westbridge': 469,\n",
       " 'fetches': 5427,\n",
       " 'hofd': 471,\n",
       " 'embellishments': 472,\n",
       " 'person': 473,\n",
       " 'wolfs': 474,\n",
       " 'customized': 475,\n",
       " 'daugther': 476,\n",
       " 'debralee': 479,\n",
       " 'lonely': 480,\n",
       " 'lutte': 481,\n",
       " 'courthouse': 485,\n",
       " 'shandling': 492,\n",
       " 'colorless': 484,\n",
       " 'neapolitan': 488,\n",
       " 'prostrate': 487,\n",
       " 'boyhood': 489,\n",
       " 'cludia': 491,\n",
       " 'chunkhead': 493,\n",
       " 'pfft': 494,\n",
       " 'burgandian': 496,\n",
       " 'fsb': 498,\n",
       " 'frill': 499,\n",
       " 'dekho': 500,\n",
       " 'dithyrambical': 501,\n",
       " 'irwin': 502,\n",
       " 'pazo': 503,\n",
       " 'refresh': 504,\n",
       " 'sacrine': 54243,\n",
       " 'refugee': 505,\n",
       " 'pepper': 41611,\n",
       " 'harnessed': 506,\n",
       " 'episopes': 508,\n",
       " 'feedback': 57413,\n",
       " 'mano': 509,\n",
       " 'mclean': 66869,\n",
       " 'batter': 510,\n",
       " 'skelter': 511,\n",
       " 'waylay': 512,\n",
       " 'enjoyment': 513,\n",
       " 'expatriates': 514,\n",
       " 'aneurism': 515,\n",
       " 'invading': 516,\n",
       " 'henkin': 517,\n",
       " 'tasteless': 31529,\n",
       " 'sssss': 518,\n",
       " 'lidia': 521,\n",
       " 'changings': 49624,\n",
       " 'jist': 522,\n",
       " 'iteration': 523,\n",
       " 'sony': 524,\n",
       " 'nickolas': 31454,\n",
       " 'scriptures': 95,\n",
       " 'stoutest': 526,\n",
       " 'kensett': 527,\n",
       " 'turnbull': 528,\n",
       " 'bluster': 52313,\n",
       " 'bsm': 529,\n",
       " 'marking': 530,\n",
       " 'valery': 51807,\n",
       " 'sizzles': 531,\n",
       " 'overtake': 532,\n",
       " 'contractually': 533,\n",
       " 'westbridbe': 535,\n",
       " 'overwhelm': 537,\n",
       " 'pasting': 538,\n",
       " 'coleridge': 540,\n",
       " 'terrors': 542,\n",
       " 'elongate': 543,\n",
       " 'elman': 65270,\n",
       " 'backtrack': 544,\n",
       " 'homes': 545,\n",
       " 'dines': 546,\n",
       " 'harrar': 547,\n",
       " 'spang': 548,\n",
       " 'bogota': 549,\n",
       " 'airport': 24843,\n",
       " 'sparkly': 550,\n",
       " 'ensemble': 551,\n",
       " 'sistine': 552,\n",
       " 'appealing': 24844,\n",
       " 'trailed': 558,\n",
       " 'museums': 557,\n",
       " 'ravi': 3620,\n",
       " 'hanged': 12364,\n",
       " 'nombre': 559,\n",
       " 'naish': 560,\n",
       " 'rhimes': 562,\n",
       " 'auditory': 563,\n",
       " 'viertel': 564,\n",
       " 'antsy': 567,\n",
       " 'petrol': 566,\n",
       " 'playmates': 568,\n",
       " 'randomly': 569,\n",
       " 'shamefull': 578,\n",
       " 'bulk': 572,\n",
       " 'confidante': 574,\n",
       " 'jacky': 577,\n",
       " 'mourby': 579,\n",
       " 'parochial': 581,\n",
       " 'rousselot': 582,\n",
       " 'chinamen': 68030,\n",
       " 'doinks': 583,\n",
       " 'crapulastic': 585,\n",
       " 'woodchuck': 587,\n",
       " 'disappointing': 588,\n",
       " 'plagiarizes': 589,\n",
       " 'hellish': 590,\n",
       " 'durang': 592,\n",
       " 'bullpen': 594,\n",
       " 'whereabouts': 596,\n",
       " 'chastened': 599,\n",
       " 'repeat': 598,\n",
       " 'exhaustively': 604,\n",
       " 'overdressed': 103,\n",
       " 'dateness': 32470,\n",
       " 'surfing': 603,\n",
       " 'kannes': 605,\n",
       " 'polley': 606,\n",
       " 'vocalist': 607,\n",
       " 'prashant': 608,\n",
       " 'corresponded': 610,\n",
       " 'torgoff': 12374,\n",
       " 'ayone': 612,\n",
       " 'atlas': 615,\n",
       " 'frivolities': 69718,\n",
       " 'grandmoffromero': 12378,\n",
       " 'deathit': 616,\n",
       " 'they': 618,\n",
       " 'indecisiveness': 24852,\n",
       " 'embellishment': 619,\n",
       " 'monumentally': 70238,\n",
       " 'syllabus': 621,\n",
       " 'ator': 622,\n",
       " 'ziabar': 109,\n",
       " 'kavner': 624,\n",
       " 'newswomen': 625,\n",
       " 'deyoung': 70751,\n",
       " 'simply': 626,\n",
       " 'cupidgrl': 630,\n",
       " 'dreads': 628,\n",
       " 'bestowing': 631,\n",
       " 'rocky': 632,\n",
       " 'mckelheer': 12381,\n",
       " 'nanny': 29119,\n",
       " 'aligning': 66166,\n",
       " 'atkinmson': 633,\n",
       " 'rushed': 31025,\n",
       " 'worshiping': 49646,\n",
       " 'avenged': 634,\n",
       " 'mouthpiece': 635,\n",
       " 'beslon': 636,\n",
       " 'atomspheric': 637,\n",
       " 'rollnecks': 640,\n",
       " 'selma': 641,\n",
       " 'hives': 51569,\n",
       " 'bytch': 73251,\n",
       " 'foolproof': 643,\n",
       " 'sadistically': 644,\n",
       " 'elrond': 645,\n",
       " 'kodokoo': 646,\n",
       " 'dragonfly': 49650,\n",
       " 'seems': 647,\n",
       " 'glen': 649,\n",
       " 'vermeer': 652,\n",
       " 'tarkovky': 651,\n",
       " 'contained': 24860,\n",
       " 'hiasashi': 653,\n",
       " 'arguments': 654,\n",
       " 'dutifully': 655,\n",
       " 'biked': 47259,\n",
       " 'christensen': 657,\n",
       " 'crach': 42148,\n",
       " 'unraveling': 658,\n",
       " 'penury': 659,\n",
       " 'detractions': 661,\n",
       " 'sponsor': 17134,\n",
       " 'dissimilar': 662,\n",
       " 'dickenson': 663,\n",
       " 'eco': 665,\n",
       " 'meghan': 72853,\n",
       " 'prevail': 666,\n",
       " 'exaltation': 667,\n",
       " 'emanuel': 671,\n",
       " 'treize': 669,\n",
       " 'introductory': 670,\n",
       " 'pilger': 673,\n",
       " 'felt': 674,\n",
       " 'painfulness': 37275,\n",
       " 'utterance': 675,\n",
       " 'nang': 676,\n",
       " 'boyd': 677,\n",
       " 'trends': 678,\n",
       " 'everet': 679,\n",
       " 'newsflash': 49660,\n",
       " 'obscurities': 680,\n",
       " 'desirous': 681,\n",
       " 'avoidances': 682,\n",
       " 'dissolving': 683,\n",
       " 'cristian': 684,\n",
       " 'birman': 685,\n",
       " 'vieght': 687,\n",
       " 'poker': 688,\n",
       " 'cowering': 12393,\n",
       " 'debilitating': 689,\n",
       " 'telefilm': 690,\n",
       " 'rebuke': 692,\n",
       " 'tuckered': 67182,\n",
       " 'insurgency': 693,\n",
       " 'texasville': 49662,\n",
       " 'stalls': 694,\n",
       " 'gadfly': 695,\n",
       " 'hurtling': 696,\n",
       " 'silencing': 697,\n",
       " 'lashed': 698,\n",
       " 'quelling': 699,\n",
       " 'harlow': 700,\n",
       " 'grueling': 123,\n",
       " 'recipient': 702,\n",
       " 'lhakpa': 703,\n",
       " 'rizzo': 704,\n",
       " 'sleeps': 705,\n",
       " 'afl': 706,\n",
       " 'injecting': 707,\n",
       " 'rectal': 708,\n",
       " 'stingy': 710,\n",
       " 'handled': 711,\n",
       " 'opposite': 712,\n",
       " 'dawned': 60005,\n",
       " 'ar': 713,\n",
       " 'dancey': 715,\n",
       " 'yorks': 716,\n",
       " 'mena': 717,\n",
       " 'zaphod': 718,\n",
       " 'mccree': 719,\n",
       " 'mosley': 720,\n",
       " 'ficker': 721,\n",
       " 'shamefacedly': 723,\n",
       " 'highschool': 725,\n",
       " 'franks': 726,\n",
       " 'redecorating': 734,\n",
       " 'inoffensive': 728,\n",
       " 'presentations': 729,\n",
       " 'chatterjee': 71609,\n",
       " 'bogging': 731,\n",
       " 'angler': 733,\n",
       " 'dolenz': 735,\n",
       " 'powerless': 736,\n",
       " 'atari': 737,\n",
       " 'syndicated': 738,\n",
       " 'meyer': 739,\n",
       " 'blinkered': 740,\n",
       " 'calculus': 741,\n",
       " 'caricaturing': 58345,\n",
       " 'drooping': 743,\n",
       " 'interdependence': 744,\n",
       " 'bhat': 745,\n",
       " 'paternal': 746,\n",
       " 'embarrasing': 747,\n",
       " 'crewman': 748,\n",
       " 'patina': 749,\n",
       " 'heterogeneity': 61934,\n",
       " 'heorot': 751,\n",
       " 'uncinematic': 754,\n",
       " 'maradona': 12664,\n",
       " 'bondage': 69178,\n",
       " 'wazoo': 755,\n",
       " 'stabbed': 12399,\n",
       " 'unpaid': 126,\n",
       " 'sokurov': 756,\n",
       " 'hancock': 757,\n",
       " 'wordiness': 758,\n",
       " 'velva': 759,\n",
       " 'lording': 61937,\n",
       " 'appereantly': 760,\n",
       " 'batboy': 761,\n",
       " 'bedrooms': 762,\n",
       " 'guiness': 763,\n",
       " 'paroled': 24879,\n",
       " 'larval': 764,\n",
       " 'plough': 65841,\n",
       " 'lubitsch': 37285,\n",
       " 'unofficially': 765,\n",
       " 'pandemoniums': 767,\n",
       " 'animatronics': 768,\n",
       " 'sailplane': 770,\n",
       " 'eisenberg': 70073,\n",
       " 'corridors': 771,\n",
       " 'lanisha': 73480,\n",
       " 'armistice': 772,\n",
       " 'walbrook': 61938,\n",
       " 'extrapolates': 773,\n",
       " 'cramer': 12406,\n",
       " 'stony': 774,\n",
       " 'plow': 776,\n",
       " 'blundered': 12407,\n",
       " 'lambaste': 777,\n",
       " 'mummifies': 779,\n",
       " 'ugghhh': 49674,\n",
       " 'disrupting': 133,\n",
       " 'catelain': 780,\n",
       " 'sematary': 65133,\n",
       " 'katherine': 47852,\n",
       " 'crumley': 782,\n",
       " 'corks': 783,\n",
       " 'sho': 15958,\n",
       " 'glom': 71295,\n",
       " 'matic': 785,\n",
       " 'moviesuntil': 70685,\n",
       " 'darts': 786,\n",
       " 'chandra': 787,\n",
       " 'janeway': 788,\n",
       " 'charlene': 789,\n",
       " 'rearise': 12410,\n",
       " 'winninger': 37293,\n",
       " 'schotland': 791,\n",
       " 'dwarves': 792,\n",
       " 'spradlin': 137,\n",
       " 'acquaintaces': 793,\n",
       " 'carnby': 795,\n",
       " 'prehysteria': 61942,\n",
       " 'psilcybe': 796,\n",
       " 'anarchistic': 56865,\n",
       " 'jefferey': 803,\n",
       " 'eardrum': 798,\n",
       " 'guarding': 799,\n",
       " 'slapped': 801,\n",
       " 'defensively': 802,\n",
       " 'attenborough': 37296,\n",
       " 'copyrighted': 806,\n",
       " 'consolation': 805,\n",
       " 'beren': 807,\n",
       " 'aesop': 810,\n",
       " 'darbar': 809,\n",
       " 'nucyaler': 813,\n",
       " 'linesman': 812,\n",
       " 'broflofski': 71297,\n",
       " 'largess': 815,\n",
       " 'hermandad': 819,\n",
       " 'cinefest': 816,\n",
       " 'emerging': 817,\n",
       " 'flat': 26613,\n",
       " 'tyrranical': 820,\n",
       " 'freddyshoop': 821,\n",
       " 'cuba': 825,\n",
       " 'cody': 824,\n",
       " 'philosophy': 826,\n",
       " 'marcello': 830,\n",
       " 'kremlin': 831,\n",
       " 'clive': 63190,\n",
       " 'mikhali': 832,\n",
       " 'stems': 833,\n",
       " 'chippendale': 834,\n",
       " 'coiffed': 835,\n",
       " 'hangdog': 49684,\n",
       " 'join': 836,\n",
       " 'unmarked': 141,\n",
       " 'escaping': 838,\n",
       " 'dubliner': 839,\n",
       " 'scriptural': 840,\n",
       " 'ravis': 842,\n",
       " 'rickshaw': 843,\n",
       " 'libidos': 845,\n",
       " 'meanial': 846,\n",
       " 'habituation': 59052,\n",
       " 'rosnelski': 847,\n",
       " 'shanghai': 69035,\n",
       " 'hestons': 848,\n",
       " 'imprisoning': 851,\n",
       " 'subhuman': 850,\n",
       " 'cappucino': 853,\n",
       " 'maare': 12417,\n",
       " 'gran': 854,\n",
       " 'lear': 855,\n",
       " 'kneecaps': 856,\n",
       " 'spinell': 24900,\n",
       " 'denotes': 858,\n",
       " 'paralelling': 866,\n",
       " 'housewife': 860,\n",
       " 'hypocrisies': 863,\n",
       " 'littlest': 862,\n",
       " 'britains': 864,\n",
       " 'baranski': 865,\n",
       " 'mis': 868,\n",
       " 'fbp': 873,\n",
       " 'unmasking': 870,\n",
       " 'montorsi': 871,\n",
       " 'yoakum': 54052,\n",
       " 'kipps': 874,\n",
       " 'cheorgraphed': 875,\n",
       " 'proprietors': 876,\n",
       " 'constitutional': 877,\n",
       " 'cashiers': 878,\n",
       " 'lusitania': 879,\n",
       " 'lamping': 881,\n",
       " 'phalke': 882,\n",
       " 'montana': 37308,\n",
       " 'pitted': 883,\n",
       " 'thirsting': 49687,\n",
       " 'giurgiu': 884,\n",
       " 'wallpaper': 885,\n",
       " 'dolf': 73170,\n",
       " 'see': 37309,\n",
       " 'kuno': 886,\n",
       " 'cherished': 887,\n",
       " 'rhyes': 889,\n",
       " 'braintrust': 892,\n",
       " 'indiain': 891,\n",
       " 'lasting': 72080,\n",
       " 'figueroa': 893,\n",
       " 'asceticism': 61958,\n",
       " 'jee': 894,\n",
       " 'migraines': 895,\n",
       " 'venantino': 897,\n",
       " 'bbca': 903,\n",
       " 'diurnal': 29331,\n",
       " 'sycophant': 902,\n",
       " 'locken': 904,\n",
       " 'tusshar': 37315,\n",
       " 'decorative': 56798,\n",
       " 'knowles': 905,\n",
       " 'mayhem': 51665,\n",
       " 'mouths': 906,\n",
       " 'abominibal': 907,\n",
       " 'nukkin': 909,\n",
       " 'icare': 910,\n",
       " 'zandalee': 911,\n",
       " 'medichlorians': 912,\n",
       " 'slyly': 913,\n",
       " 'swimming': 37318,\n",
       " 'degradable': 61963,\n",
       " 'pp': 914,\n",
       " 'tawny': 915,\n",
       " 'rdm': 916,\n",
       " 'scuttle': 48473,\n",
       " 'cameraderie': 917,\n",
       " 'godless': 918,\n",
       " 'codename': 919,\n",
       " 'sothern': 53729,\n",
       " 'vadar': 12433,\n",
       " 'jyada': 920,\n",
       " 'gospels': 921,\n",
       " 'hatches': 922,\n",
       " 'bronte': 923,\n",
       " 'anneliza': 924,\n",
       " 'upriver': 37321,\n",
       " 'uwe': 24911,\n",
       " 'relieved': 163,\n",
       " 'journalistic': 925,\n",
       " 'del': 926,\n",
       " 'intentional': 160,\n",
       " 'petered': 929,\n",
       " 'mcbrde': 930,\n",
       " 'hedonism': 931,\n",
       " 'peevish': 57821,\n",
       " 'wayan': 934,\n",
       " 'jeux': 935,\n",
       " 'craftily': 49693,\n",
       " 'pressuring': 936,\n",
       " 'reputed': 937,\n",
       " 'depress': 938,\n",
       " 'sensationalist': 939,\n",
       " 'guhther': 940,\n",
       " 'cringeable': 68201,\n",
       " 'wolfen': 941,\n",
       " 'lynx': 942,\n",
       " 'summerisle': 949,\n",
       " 'saban': 12439,\n",
       " 'fliers': 945,\n",
       " 'tiltes': 947,\n",
       " 'cicely': 951,\n",
       " 'eradication': 948,\n",
       " 'dangles': 950,\n",
       " 'blasted': 952,\n",
       " 'gluing': 953,\n",
       " 'dailys': 955,\n",
       " 'summoning': 956,\n",
       " 'underplays': 957,\n",
       " 'gorns': 958,\n",
       " 'decontamination': 45487,\n",
       " 'epiphanal': 48014,\n",
       " 'clunker': 959,\n",
       " 'edgier': 960,\n",
       " 'brittle': 961,\n",
       " 'disbanding': 962,\n",
       " 'heuristic': 963,\n",
       " 'walkleys': 49697,\n",
       " 'wynorski': 967,\n",
       " 'digest': 965,\n",
       " 'prohibited': 968,\n",
       " 'casars': 970,\n",
       " 'thompson': 24919,\n",
       " 'extremes': 73165,\n",
       " 'enhancer': 971,\n",
       " 'tungtvannet': 972,\n",
       " 'kruegar': 72565,\n",
       " 'jiggling': 61973,\n",
       " 'coppery': 171,\n",
       " 'unsubtly': 975,\n",
       " 'hamilton': 976,\n",
       " 'tollywood': 24923,\n",
       " 'backstories': 49700,\n",
       " 'broads': 977,\n",
       " 'slither': 28918,\n",
       " 'pies': 978,\n",
       " 'invetigator': 979,\n",
       " 'powerful': 61471,\n",
       " 'disturb': 980,\n",
       " 'mortensen': 981,\n",
       " 'jaku': 982,\n",
       " 'dialects': 44111,\n",
       " 'affectts': 983,\n",
       " 'diwana': 986,\n",
       " 'sala': 985,\n",
       " 'bathebo': 987,\n",
       " 'typing': 988,\n",
       " 'lise': 989,\n",
       " 'adoration': 37329,\n",
       " 'barriers': 991,\n",
       " 'acteurs': 49702,\n",
       " 'jupiter': 995,\n",
       " 'inflict': 994,\n",
       " 'cadaverous': 998,\n",
       " 'splint': 996,\n",
       " 'elsons': 178,\n",
       " 'chatsworth': 1000,\n",
       " 'harboured': 1001,\n",
       " 'implode': 1002,\n",
       " 'beluche': 177,\n",
       " 'terada': 1003,\n",
       " 'morand': 1008,\n",
       " 'hodgins': 1005,\n",
       " 'whiteys': 1007,\n",
       " 'conceptionless': 1012,\n",
       " ...}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_to_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the labels\n",
    "\n",
    "Our labels are \"positive\" or \"negative\". To use these labels in our network, we need to convert them to 0 and 1.\n",
    "\n",
    "> **Exercise:** Convert labels from `positive` and `negative` to 1 and 0, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " ...]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'negative',\n",
       " ...]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(labels.split('\\n')).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {'positive': 1, 'negative':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_ints = []\n",
    "labels_ints.append([label_dict[k] for k in ' '.join(labels.split('\\n')).split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to 1s and 0s for 'positive' and 'negative'\n",
    "labels = labels_ints.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you built `labels` correctly, you should see the next output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews: 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-3885c393e92b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mreview_lens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreviews_ints\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Zero-length reviews: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview_lens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Maximum review length: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "review_lens = Counter([len(x) for x in reviews_ints])\n",
    "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
    "print(\"Maximum review length: {}\".format(max(review_lens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, a couple issues here. We seem to have one review with zero length. And, the maximum review length is way too many steps for our RNN. Let's truncate to 200 steps. For reviews shorter than 200, we'll pad with 0s. For reviews longer than 200, we can truncate them to the first 200 characters.\n",
    "\n",
    "> **Exercise:** First, remove the review with zero length from the `reviews_ints` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out that review with 0 length\n",
    "reviews_ints = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** Now, create an array `features` that contains the data we'll pass to the network. The data should come from `review_ints`, since we want to feed integers to the network. Each row should be 200 elements long. For reviews shorter than 200 words, left pad with 0s. That is, if the review is `['best', 'movie', 'ever']`, `[117, 18, 128]` as integers, the row will look like `[0, 0, 0, ..., 0, 117, 18, 128]`. For reviews longer than 200, use on the first 200 words as the feature vector.\n",
    "\n",
    "This isn't trivial and there are a bunch of ways to do this. But, if you're going to be building your own deep learning networks, you're going to have to get used to preparing your data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 200\n",
    "features = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you build features correctly, it should look like that cell output below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0, 21282,   308,     6,\n",
       "            3,  1050,   207,     8,  2143,    32,     1,   171,    57,\n",
       "           15,    49,    81,  5832,    44,   382,   110,   140,    15,\n",
       "         5236,    60,   154,     9,     1,  5014,  5899,   475,    71,\n",
       "            5,   260,    12, 21282,   308,    13,  1981,     6,    74,\n",
       "         2396],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,    63,     4,     3,   125,\n",
       "           36,    47,  7487,  1397,    16,     3,  4212,   505,    45,\n",
       "           17],\n",
       "       [23249,    42, 55982,    15,   707, 17288,  3398,    47,    77,\n",
       "           35,  1822,    16,   154,    19,   114,     3,  1308,     5,\n",
       "          336,   147,    22,     1,   857,    12,    70,   281,  1170,\n",
       "          399,    36,   120,   283,    38,   169,     5,   382,   158,\n",
       "           42,  2278,    16,     1,   541,    90,    78,   102,     4,\n",
       "            1,  3256,    15,    43,     3,   407,  1069,   136,  8165,\n",
       "           44,   182,   140,    15,  3054,     1,   321,    22,  4827,\n",
       "        28571,   346,     5,  3093,  2094,     1, 18970, 18062,    42,\n",
       "         8165,    46,    33,   236,    29,   370,     5,   130,    56,\n",
       "           22,     1,  1928,     7,     7,    19,    48,    46,    21,\n",
       "           70,   345,     3,  2102,     5,   408,    22,     1,  1928,\n",
       "           16],\n",
       "       [ 4504,   505,    15,     3,  3352,   162,  8369,  1655,     6,\n",
       "         4860,    56,    17,  4513,  5629,   140, 11938,     5,   996,\n",
       "         4969,  2947,  4464,   566,  1202,    36,     6,  1520,    96,\n",
       "            3,   744,     4, 28265,    13,     5,    27,  3464,     9,\n",
       "        10794,     4,     8,   111,  3024,     5,     1,  1027,    15,\n",
       "            3,  4400,    82,    22,  2058,     6,  4464,   538,  2773,\n",
       "         7123, 41932,    41,   463,     1,  8369, 62989,   302,   123,\n",
       "           15,  4230,    19,  1671,   923,     1,  1655,     6,  6166,\n",
       "        20692,    34,     1,   980,  1751, 22804,   646, 25292,    27,\n",
       "          106, 11901,    13, 14278, 15496, 18701,  2459,   466, 21189,\n",
       "           36,  3267,     1,  6436,  1020,    45,    17,  2700,  2500,\n",
       "           33],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,   520,   119,   113,    34,\n",
       "        16673,  1817,  3744,   117,   885, 22019,   721,    10,    28,\n",
       "          124,   108,     2,   115,   137,     9,  1626,  7742,    26,\n",
       "          330,     5,   590,     1,  6165,    22,   386,     6,     3,\n",
       "          349,    15,    50,    15,   231,     9,  7484, 11646,     1,\n",
       "          191,    22,  8994,     6,    82,   881,   101,   111,  3590,\n",
       "            4],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           11,    20,  3662,   141,    10,   423,    23,   272,    60,\n",
       "         4362,    22,    32,    84,  3305,    22,     1,   172,     4,\n",
       "            1,   952,   507,    11,  4996,  5387,     5,   574,     4,\n",
       "         1154,    54,    53,  5328,     1,   261,    17,    41,   952,\n",
       "          125,    59,     1,   712,   137,   379,   627,    15,   111,\n",
       "         1511],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,    11,     6,   692,     1,    90,\n",
       "         2158,    20, 11793,     1,  2818,  5218,   249,    92,  3007,\n",
       "            8,   126,    24,   201,     3,   803,   634,     4, 23249,\n",
       "         1002],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,   786,   295,    10,   122,    11,     6,   418,\n",
       "            5,    29,    35,   482,    20,    19,  1285,    33,   142,\n",
       "           28,  2667,    45,  1844,    32,     1,  2790,    37,    78,\n",
       "           97,  2439,    67,  3952,    45,     2,    24,   105,   256,\n",
       "            1,   134,  1572,     2, 12612,   452,    14,   319,    11,\n",
       "           63,     6,    98,  1322,     5,   105,     1,  3766,     4,\n",
       "            3],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,    11,     6,\n",
       "           24,     1,   779,  3708,  2818,    20,     8,    14,    74,\n",
       "          325,  2739,    73,    90,     4,    27,    99,     2,   165,\n",
       "           68],\n",
       "       [   54,    10,    14,   116,    60,   798,   552,    71,   364,\n",
       "            5,     1,   731,     5,    66,  8089,     8,    14,    30,\n",
       "            4,   109,    99,    10,   293,    17,    60,   798,    19,\n",
       "           11,    14,     1,    64,    30,    69,  2506,    45,     4,\n",
       "          234,    93,    10,    68,   114,   108,  8089,   363,    43,\n",
       "         1009,     2,    10,    97,    28,  1431,    45,     1,   357,\n",
       "            4,    60,   110,   205,     8,    48,     3,  1929, 11029,\n",
       "            2,  2127,   354,   412,     4,    13,  6676,     2,  2975,\n",
       "         5174,  2125,  1371,     6,    30,     4,    60,   502,   876,\n",
       "           19,  8089,     6,    34,   227,     1,   247,   412,     4,\n",
       "          582,     4,    27,   599,     9,     1, 13829,   395,     4,\n",
       "        14175]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[:10,:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Validation, Test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data in nice shape, we'll split it into training, validation, and test sets.\n",
    "\n",
    "> **Exercise:** Create the training, validation, and test sets here. You'll need to create sets for the features and the labels, `train_x` and `train_y` for example. Define a split fraction, `split_frac` as the fraction of data to keep in the training set. Usually this is set to 0.8 or 0.9. The rest of the data will be split in half to create the validation and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_frac = 0.8\n",
    "\n",
    "train_x, val_x = \n",
    "train_y, val_y = \n",
    "\n",
    "val_x, test_x = \n",
    "val_y, test_y = \n",
    "\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With train, validation, and text fractions of 0.8, 0.1, 0.1, the final shapes should look like:\n",
    "```\n",
    "                    Feature Shapes:\n",
    "Train set: \t\t (20000, 200) \n",
    "Validation set: \t(2500, 200) \n",
    "Test set: \t\t  (2500, 200)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the graph\n",
    "\n",
    "Here, we'll build the graph. First up, defining the hyperparameters.\n",
    "\n",
    "* `lstm_size`: Number of units in the hidden layers in the LSTM cells. Usually larger is better performance wise. Common values are 128, 256, 512, etc.\n",
    "* `lstm_layers`: Number of LSTM layers in the network. I'd start with 1, then add more if I'm underfitting.\n",
    "* `batch_size`: The number of reviews to feed the network in one training pass. Typically this should be set as high as you can go without running out of memory.\n",
    "* `learning_rate`: Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_size = 256\n",
    "lstm_layers = 1\n",
    "batch_size = 500\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the network itself, we'll be passing in our 200 element long review vectors. Each batch will be `batch_size` vectors. We'll also be using dropout on the LSTM layer, so we'll make a placeholder for the keep probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** Create the `inputs_`, `labels_`, and drop out `keep_prob` placeholders using `tf.placeholder`. `labels_` needs to be two-dimensional to work with some functions later.  Since `keep_prob` is a scalar (a 0-dimensional tensor), you shouldn't provide a size to `tf.placeholder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = len(vocab_to_int) + 1 # Adding 1 because we use 0's for padding, dictionary started at 1\n",
    "\n",
    "# Create the graph object\n",
    "graph = tf.Graph()\n",
    "# Add nodes to the graph\n",
    "with graph.as_default():\n",
    "    inputs_ = \n",
    "    labels_ = \n",
    "    keep_prob = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "\n",
    "Now we'll add an embedding layer. We need to do this because there are 74000 words in our vocabulary. It is massively inefficient to one-hot encode our classes here. You should remember dealing with this problem from the word2vec lesson. Instead of one-hot encoding, we can have an embedding layer and use that layer as a lookup table. You could train an embedding layer using word2vec, then load it here. But, it's fine to just make a new layer and let the network learn the weights.\n",
    "\n",
    "> **Exercise:** Create the embedding lookup matrix as a `tf.Variable`. Use that embedding matrix to get the embedded vectors to pass to the LSTM cell with [`tf.nn.embedding_lookup`](https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup). This function takes the embedding matrix and an input tensor, such as the review vectors. Then, it'll return another tensor with the embedded vectors. So, if the embedding layer has 200 units, the function will return a tensor with size [batch_size, 200].\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the embedding vectors (number of units in the embedding layer)\n",
    "embed_size = 300 \n",
    "\n",
    "with graph.as_default():\n",
    "    embedding = \n",
    "    embed = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM cell\n",
    "\n",
    "<img src=\"assets/network_diagram.png\" width=400px>\n",
    "\n",
    "Next, we'll create our LSTM cells to use in the recurrent network ([TensorFlow documentation](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn)). Here we are just defining what the cells look like. This isn't actually building the graph, just defining the type of cells we want in our graph.\n",
    "\n",
    "To create a basic LSTM cell for the graph, you'll want to use `tf.contrib.rnn.BasicLSTMCell`. Looking at the function documentation:\n",
    "\n",
    "```\n",
    "tf.contrib.rnn.BasicLSTMCell(num_units, forget_bias=1.0, input_size=None, state_is_tuple=True, activation=<function tanh at 0x109f1ef28>)\n",
    "```\n",
    "\n",
    "you can see it takes a parameter called `num_units`, the number of units in the cell, called `lstm_size` in this code. So then, you can write something like \n",
    "\n",
    "```\n",
    "lstm = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "```\n",
    "\n",
    "to create an LSTM cell with `num_units`. Next, you can add dropout to the cell with `tf.contrib.rnn.DropoutWrapper`. This just wraps the cell in another cell, but with dropout added to the inputs and/or outputs. It's a really convenient way to make your network better with almost no effort! So you'd do something like\n",
    "\n",
    "```\n",
    "drop = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
    "```\n",
    "\n",
    "Most of the time, your network will have better performance with more layers. That's sort of the magic of deep learning, adding more layers allows the network to learn really complex relationships. Again, there is a simple way to create multiple layers of LSTM cells with `tf.contrib.rnn.MultiRNNCell`:\n",
    "\n",
    "```\n",
    "cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)\n",
    "```\n",
    "\n",
    "Here, `[drop] * lstm_layers` creates a list of cells (`drop`) that is `lstm_layers` long. The `MultiRNNCell` wrapper builds this into multiple layers of RNN cells, one for each cell in the list.\n",
    "\n",
    "So the final cell you're using in the network is actually multiple (or just one) LSTM cells with dropout. But it all works the same from an achitectural viewpoint, just a more complicated graph in the cell.\n",
    "\n",
    "> **Exercise:** Below, use `tf.contrib.rnn.BasicLSTMCell` to create an LSTM cell. Then, add drop out to it with `tf.contrib.rnn.DropoutWrapper`. Finally, create multiple LSTM layers with `tf.contrib.rnn.MultiRNNCell`.\n",
    "\n",
    "Here is [a tutorial on building RNNs](https://www.tensorflow.org/tutorials/recurrent) that will help you out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    # Your basic LSTM cell\n",
    "    lstm = \n",
    "    \n",
    "    # Add dropout to the cell\n",
    "    drop = \n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = \n",
    "    \n",
    "    # Getting an initial state of all zeros\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN forward pass\n",
    "\n",
    "<img src=\"assets/network_diagram.png\" width=400px>\n",
    "\n",
    "Now we need to actually run the data through the RNN nodes. You can use [`tf.nn.dynamic_rnn`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn) to do this. You'd pass in the RNN cell you created (our multiple layered LSTM `cell` for instance), and the inputs to the network.\n",
    "\n",
    "```\n",
    "outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, initial_state=initial_state)\n",
    "```\n",
    "\n",
    "Above I created an initial state, `initial_state`, to pass to the RNN. This is the cell state that is passed between the hidden layers in successive time steps. `tf.nn.dynamic_rnn` takes care of most of the work for us. We pass in our cell and the input to the cell, then it does the unrolling and everything else for us. It returns outputs for each time step and the final_state of the hidden layer.\n",
    "\n",
    "> **Exercise:** Use `tf.nn.dynamic_rnn` to add the forward pass through the RNN. Remember that we're actually passing in vectors from the embedding layer, `embed`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    outputs, final_state = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output\n",
    "\n",
    "We only care about the final output, we'll be using that as our sentiment prediction. So we need to grab the last output with `outputs[:, -1]`, the calculate the cost from that and `labels_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    predictions = tf.contrib.layers.fully_connected(outputs[:, -1], 1, activation_fn=tf.sigmoid)\n",
    "    cost = tf.losses.mean_squared_error(labels_, predictions)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation accuracy\n",
    "\n",
    "Here we can add a few nodes to calculate the accuracy which we'll use in the validation pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching\n",
    "\n",
    "This is a simple function for returning batches from our data. First it removes data such that we only have full batches. Then it iterates through the `x` and `y` arrays and returns slices out of those arrays with size `[batch_size]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size=100):\n",
    "    \n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Below is the typical training code. If you want to do this yourself, feel free to delete all this code and implement it yourself. Before you run this, make sure the `checkpoints` directory exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "    for e in range(epochs):\n",
    "        state = sess.run(initial_state)\n",
    "        \n",
    "        for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):\n",
    "            feed = {inputs_: x,\n",
    "                    labels_: y[:, None],\n",
    "                    keep_prob: 0.5,\n",
    "                    initial_state: state}\n",
    "            loss, state, _ = sess.run([cost, final_state, optimizer], feed_dict=feed)\n",
    "            \n",
    "            if iteration%5==0:\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Train loss: {:.3f}\".format(loss))\n",
    "\n",
    "            if iteration%25==0:\n",
    "                val_acc = []\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                for x, y in get_batches(val_x, val_y, batch_size):\n",
    "                    feed = {inputs_: x,\n",
    "                            labels_: y[:, None],\n",
    "                            keep_prob: 1,\n",
    "                            initial_state: val_state}\n",
    "                    batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                    val_acc.append(batch_acc)\n",
    "                print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "            iteration +=1\n",
    "    saver.save(sess, \"checkpoints/sentiment.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = []\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    for ii, (x, y) in enumerate(get_batches(test_x, test_y, batch_size), 1):\n",
    "        feed = {inputs_: x,\n",
    "                labels_: y[:, None],\n",
    "                keep_prob: 1,\n",
    "                initial_state: test_state}\n",
    "        batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "        test_acc.append(batch_acc)\n",
    "    print(\"Test accuracy: {:.3f}\".format(np.mean(test_acc)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
